{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6626821-4db1-47bc-8952-8219d9556426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9052d3e-b97d-448d-8d20-69b2772ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531e2ff-02f8-4bb9-8d0d-629de241cb8c",
   "metadata": {},
   "source": [
    "# Install and load all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b233ca7a-64b9-4799-b02f-0650033bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "from skimage.measure import label,regionprops,find_contours\n",
    "from evalutils import DetectionAlgorithm\n",
    "from evalutils.validators import UniquePathIndicesValidator,DataFrameValidator\n",
    "from evalutils.exceptions import ValidationError\n",
    "import json, random, SimpleITK, gc, cv2\n",
    "from typing import Tuple, Dict\n",
    "from pandas import DataFrame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb1c8c2-a278-4a9c-be23-2f2f723fc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0512f7ea-b726-46bd-becf-712357b7e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def splitter(df):\n",
    "    train = df.index[~df['valid']].to_list()\n",
    "    valid = df.index[df['valid']].to_list()\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f2b136-6755-42b2-a5d9-554fd3127026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "vocab=L([['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c53458e-eac9-44dc-bf82-ab11ab6c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "c=L([len(v) for v in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c109919f-3177-427d-8058-15b72328459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def cfg (i): return c[:i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37637a38-2e16-4740-8972-d6a14d27a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# defining error rate for each robotic hand tools\n",
    "def usm1_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,:cfg(1)], usm1_targs)\n",
    "def usm2_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(1):cfg(2)], usm2_targs)\n",
    "def usm3_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(2):cfg(3)], usm3_targs)\n",
    "def usm4_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(3):cfg(4)], usm4_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9aadb91-5e93-496c-9740-01420f9164ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# defining combined error rate \n",
    "def combo_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): \n",
    "    return usm1_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm2_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm3_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm4_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d5da0b-4101-4db3-8718-2445b2aea548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# defining error rate for each robotic hand tools for raw preds from the learner \n",
    "def usm1_err_raw(preds,targs): return error_rate(preds[:,:cfg(1)].softmax(dim=1).argmax(dim=1), targs)\n",
    "def usm2_err_raw(preds,targs): return error_rate(preds[:,cfg(1):cfg(2)].softmax(dim=1).argmax(dim=1), targs)\n",
    "def usm3_err_raw(preds,targs): return error_rate(preds[:,cfg(2):cfg(3)].softmax(dim=1).argmax(dim=1), targs)\n",
    "def usm4_err_raw(preds,targs): return error_rate(preds[:,cfg(3):cfg(4)].softmax(dim=1).argmax(dim=1), targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6737c45a-38c4-495e-ad0f-cd2da7c3c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# defining loss function for each robotic hand tools\n",
    "def usm1_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,:cfg(1)], usm1_targs,**kwargs)\n",
    "def usm2_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(1):cfg(2)], usm2_targs,**kwargs)\n",
    "def usm3_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(2):cfg(3)], usm3_targs,**kwargs)\n",
    "def usm4_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(3):cfg(4)], usm4_targs,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756b250c-0bac-49ac-8a64-f6675deed72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# defining combined loss\n",
    "def combo_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): \n",
    "    return usm1_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm2_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm3_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm4_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27645e9-8cc6-476e-a5cf-818b2b6533d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# configuring metrics and loss for learner\n",
    "metrics_cfg = [usm1_loss,usm2_loss,usm3_loss,usm4_loss,usm1_err,usm2_err,usm3_err,usm4_err, combo_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467b716b-0f8c-45b0-b7f5-d7d94b725f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# error rate fns for inference and validation\n",
    "def usm_err_raw(preds,targs): return error_rate(preds, targs)\n",
    "def combo_err_raw(preds, targs): \n",
    "    return usm_err_raw(preds[:,:cfg(1)].softmax(dim=1),targs[0])+usm_err_raw(preds[:,cfg(1):cfg(2)].softmax(dim=1),targs[1])+usm_err_raw(preds[:,cfg(2):cfg(3)].softmax(dim=1),targs[2])+usm_err_raw(preds[:,cfg(3):cfg(4)].softmax(dim=1),targs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8da3501-50d9-4951-bbb3-cceb9a9a15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def get_image_mask(fn):\n",
    "    f=Path(str(fn).replace('images', 'masks').replace('jpg','png'))\n",
    "    return PILMask.create(f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8cf25f-bd4a-4bfb-bba9-06090994b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def custom_accuracy(inp, targ):\n",
    "    targ = targ.squeeze(1)\n",
    "    return (inp.argmax(dim=1)==targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0822e98e-a00b-458c-9f6a-3613a6ec71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "execute_in_docker = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9142a2e-75d0-4f02-92ae-3ceaa27dcd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VideoLoader():\n",
    "    def load(self, *, fname):\n",
    "        path = Path(fname)\n",
    "        print(path)\n",
    "        if not path.is_file():\n",
    "            raise IOError(f\"Could not load {fname} using {self.__class__.__qualname__}.\")\n",
    "        return [{\"path\": fname}]\n",
    "\n",
    "    # only path valid\n",
    "    def hash_video(self, input_video):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8b0ec55-355f-4078-ab12-ea8c070d4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class UniqueVideoValidator(DataFrameValidator):\n",
    "    \"\"\"\n",
    "    Validates that each video in the set is unique\n",
    "    \"\"\"\n",
    "\n",
    "    def validate(self, *, df: DataFrame):\n",
    "        try:\n",
    "            hashes = df[\"video\"]\n",
    "        except KeyError:\n",
    "            raise ValidationError(\"Column `video` not found in DataFrame.\")\n",
    "\n",
    "        if len(set(hashes)) != len(hashes):\n",
    "            raise ValidationError(\"The videos are not unique, please submit a unique video for each case.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3305081-6d9a-4d24-9011-48aeb16ccbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/input/vid_1_short.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': './test/input/vid_1_short.mp4'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=VideoLoader()\n",
    "v.load(fname='./test/input/vid_1_short.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b05c36-ad78-4d2d-b2c8-2a3182257cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Surgtoolloc_det(DetectionAlgorithm):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            index_key='input_video',\n",
    "            file_loaders={'input_video': VideoLoader()},\n",
    "            input_path=Path(\"/input/\") if execute_in_docker else Path(\"./test/input/\"),\n",
    "            output_file=Path(\"/output/surgical-tool-presence.json\") if execute_in_docker else Path(\n",
    "                \"./test/output/surgical-tool-presence.json\"),\n",
    "            validators=dict(input_video=(UniquePathIndicesValidator(),)),\n",
    "        )\n",
    "\n",
    "        # loading ensemble learner\n",
    "        ensem_path=Path('/opt/algorithm/models/cls') if execute_in_docker else Path(\"test/algorithm/cls\")\n",
    "        segmen_path=Path('/opt/algorithm/models/seg') if execute_in_docker else Path(\"test/algorithm/seg\")\n",
    "        self.ensem_learner=[load_learner(m, cpu=False) for m in ensem_path.ls() if m.suffix=='.pkl']\n",
    "        self.crop_learner=load_learner(segmen_path/'seg_v1.pkl', cpu=False)\n",
    "        self.codes = [\"Background\", \"Foreground\"]\n",
    "\n",
    "        self.tool_list = [\"needle_driver\",\n",
    "                          \"monopolar_curved_scissor\",\n",
    "                          \"force_bipolar\",\n",
    "                          \"clip_applier\",\n",
    "                          \"tip_up_fenestrated_grasper\",\n",
    "                          \"cadiere_forceps\",\n",
    "                          \"bipolar_forceps\",\n",
    "                          \"vessel_sealer\",\n",
    "                          \"suction_irrigator\",\n",
    "                          \"bipolar_dissector\",\n",
    "                          \"prograsp_forceps\",\n",
    "                          \"stapler\",\n",
    "                          \"permanent_cautery_hook_spatula\",\n",
    "                          \"grasping_retractor\"]\n",
    "\n",
    "    vocab=L([['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer'],['bipolar_dissector', 'bipolar_forceps', 'blank', 'cadiere_forceps', 'clip_applier', 'force_bipolar', 'grasping_retractor', 'monopolar_curved_scissor', 'nan', 'needle_driver', 'permanent_cautery_hook_spatula', 'prograsp_forceps', 'stapler', 'suction_irrigator', 'tip_up_fenestrated_grasper', 'vessel_sealer']])\n",
    "\n",
    "    c=L([len(v) for v in vocab])\n",
    "\n",
    "    @staticmethod\n",
    "    def splitter(df):\n",
    "        train = df.index[~df['valid']].to_list()\n",
    "        valid = df.index[df['valid']].to_list()\n",
    "        return train, valid\n",
    "    \n",
    "    @staticmethod\n",
    "    def cfg (i): return c[:i].sum()\n",
    "\n",
    "    # defining error rate for each robotic hand tools\n",
    "    @staticmethod\n",
    "    def usm1_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,:cfg(1)], usm1_targs)\n",
    "\n",
    "    @staticmethod\n",
    "    def usm2_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(1):cfg(2)], usm2_targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm3_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(2):cfg(3)], usm3_targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm4_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): return error_rate(preds[:,cfg(3):cfg(4)], usm4_targs)\n",
    "\n",
    "    # defining combined error rate \n",
    "    @staticmethod\n",
    "    def combo_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs): \n",
    "        return usm1_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm2_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm3_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)+usm4_err(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs)\n",
    "\n",
    "    # defining error rate for each robotic hand tools for raw preds from the learner \n",
    "    @staticmethod\n",
    "    def usm1_err_raw(preds,targs): return error_rate(preds[:,:cfg(1)].softmax(dim=1).argmax(dim=1), targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm2_err_raw(preds,targs): return error_rate(preds[:,cfg(1):cfg(2)].softmax(dim=1).argmax(dim=1), targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm3_err_raw(preds,targs): return error_rate(preds[:,cfg(2):cfg(3)].softmax(dim=1).argmax(dim=1), targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm4_err_raw(preds,targs): return error_rate(preds[:,cfg(3):cfg(4)].softmax(dim=1).argmax(dim=1), targs)\n",
    "\n",
    "    # defining loss function for each robotic hand tools\n",
    "    @staticmethod\n",
    "    def usm1_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,:cfg(1)], usm1_targs,**kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm2_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(1):cfg(2)], usm2_targs,**kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm3_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(2):cfg(3)], usm3_targs,**kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm4_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): return CrossEntropyLossFlat(reduction='mean')(preds[:,cfg(3):cfg(4)], usm4_targs,**kwargs)\n",
    "\n",
    "    # defining combined loss\n",
    "    @staticmethod\n",
    "    def combo_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs): \n",
    "        return usm1_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm2_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm3_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)+usm4_loss(preds,usm1_targs,usm2_targs,usm3_targs,usm4_targs,**kwargs)\n",
    "\n",
    "    # configuring metrics and loss for learner\n",
    "    metrics_cfg = [usm1_loss,usm2_loss,usm3_loss,usm4_loss,usm1_err,usm2_err,usm3_err,usm4_err, combo_err]\n",
    "\n",
    "    # error rate fns for inference and validation\n",
    "    \n",
    "    @staticmethod\n",
    "    def usm_err_raw(preds,targs): return error_rate(preds, targs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def combo_err_raw(preds, targs): \n",
    "        return usm_err_raw(preds[:,:cfg(1)].softmax(dim=1),targs[0])+usm_err_raw(preds[:,cfg(1):cfg(2)].softmax(dim=1),targs[1])+usm_err_raw(preds[:,cfg(2):cfg(3)].softmax(dim=1),targs[2])+usm_err_raw(preds[:,cfg(3):cfg(4)].softmax(dim=1),targs[3])\n",
    "\n",
    "    # \n",
    "    @staticmethod\n",
    "    def get_image_mask(fn):\n",
    "        f=Path(str(fn).replace('images', 'masks').replace('jpg','png'))\n",
    "        return PILMask.create(f) \n",
    "    #\n",
    "    @staticmethod\n",
    "    def custom_accuracy(inp, targ):\n",
    "        targ = targ.squeeze(1)\n",
    "        return (inp.argmax(dim=1)==targ).float().mean()\n",
    "\n",
    "    def crop_images(self, src):\n",
    "        fs=get_image_files(src)\n",
    "        preds,_ = self.crop_learner.get_preds(dl=self.crop_learner.dls.test_dl(fs))\n",
    "        for p, f in zip(preds,self.crop_learner.dl.items):\n",
    "\n",
    "            fn = f.name\n",
    "\n",
    "            im=PILImage.create(f)\n",
    "            (h,w)=im.shape\n",
    "            mask=PILMask.create((np.array(p.argmax(0))*255).astype(np.uint8))\n",
    "            mask=Resize((h,w), ResizeMethod.Squish) (mask)\n",
    "\n",
    "            lbl = label(np.array(mask))\n",
    "            props = regionprops(lbl)\n",
    "            x1,y1,x2,y2=props[0].bbox[0],props[0].bbox[2],props[0].bbox[1],props[0].bbox[3]\n",
    "\n",
    "            im_c = PILImage.create(np.array(im)[x1:y1,x2:y2])\n",
    "            im_c.save(src/fn)\n",
    "    \n",
    "    def extract_images(self, video_file):     \n",
    "    \n",
    "        # start the loop\n",
    "        count = 0\n",
    "        src=Path(self._input_path)\n",
    "        \n",
    "        for i in get_image_files(src): os.remove(i) \n",
    "        \n",
    "        # read the video file    \n",
    "        cap = cv2.VideoCapture(str(src/video_file))\n",
    "        \n",
    "        while True:\n",
    "            is_read, f = cap.read()\n",
    "            if not is_read:\n",
    "                # break out of the loop if there are no frames to read\n",
    "                break\n",
    "            name = str(src/f'{count}.jpg')\n",
    "            cv2.imwrite(name,f)\n",
    "            count+=1\n",
    "        cap.release()\n",
    "\n",
    "    def tool_detect_json_sample(self):\n",
    "        # single output dict\n",
    "        slice_dict = {\"slice_nr\": 1}\n",
    "        tool_boolean_dict = {i: False for i in self.tool_list}\n",
    "        single_output_dict = {**slice_dict, **tool_boolean_dict}\n",
    "        return single_output_dict\n",
    "\n",
    "    def process_case(self, *, idx, case):\n",
    "\n",
    "        # Input video would return the collection of all frames (cap object)\n",
    "        input_video_file_path = case #VideoLoader.load(case)\n",
    "        # Detect and score candidates\n",
    "        scored_candidates = self.predict(case.path) #video file > load evalutils.py\n",
    "\n",
    "        # return\n",
    "        # Write resulting candidates to result.json for this case\n",
    "        return scored_candidates\n",
    "\n",
    "    def save(self):\n",
    "        with open(str(self._output_file), \"w\") as f:\n",
    "            json.dump(self._case_results[0], f)\n",
    "\n",
    "\n",
    "    def predict(self, fname) -> Dict:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        fname -> video file path\n",
    "        \n",
    "        Output:\n",
    "        tools -> list of prediction dictionaries (per frame) in the correct format as described in documentation \n",
    "        \"\"\"\n",
    "        \n",
    "        print('Loading, extracting and cropping video file: ' + str(fname))\n",
    "        self.extract_images(fname)\n",
    "        self.crop_images(self._input_path)\n",
    "\n",
    "        fs=get_image_files(self._input_path)\n",
    "        \n",
    "        num_frames = len(fs)\n",
    "        \n",
    "        ###                                                                     ###\n",
    "        ###  TODO: adapt the following part for YOUR submission: make prediction\n",
    "        ###                                                                     ###\n",
    "        \n",
    "        print(num_frames)\n",
    "\n",
    "        # generate output json\n",
    "        all_frames_predicted_outputs = []\n",
    "        all_undefined_tools=[]\n",
    "        \n",
    "        tta_res=[]\n",
    "        prs_items=[]\n",
    "        for learn in self.ensem_learner:\n",
    "            tta_res.append(learn.tta(dl=learn.dls.test_dl(fs)))\n",
    "            if len(prs_items)<1:\n",
    "                prs_items=learn.dl.items\n",
    "\n",
    "        tta_prs=first(zip(*tta_res))\n",
    "        tta_prs+=tta_prs[:1]\n",
    "        tta_prs=torch.stack(tta_prs)\n",
    "\n",
    "        lbls=[]\n",
    "        for i in range(len(c)):\n",
    "            arm_preds = tta_prs[:,:,cfg(i):cfg(i+1)].mean(0);\n",
    "            arm_idxs = arm_preds.argmax(dim=1)\n",
    "            arm_vocab = np.array(vocab[i])\n",
    "            lbls.append(arm_vocab[arm_idxs])\n",
    "\n",
    "        for usm1,usm2,usm3,usm4,f in zip(lbls[0],lbls[1],lbls[2],lbls[3],prs_items):\n",
    "            frame_dict=self.tool_detect_json_sample()\n",
    "            frame_dict['slice_nr']=int(f.stem)\n",
    "            frame_dict[usm1]=True if usm1 in frame_dict.keys() else all_undefined_tools.append(usm1)\n",
    "            frame_dict[usm2]=True if usm2 in frame_dict.keys() else all_undefined_tools.append(usm2)\n",
    "            frame_dict[usm3]=True if usm3 in frame_dict.keys() else all_undefined_tools.append(usm3)\n",
    "            frame_dict[usm4]=True if usm4 in frame_dict.keys() else all_undefined_tools.append(usm4)\n",
    "            frame_dict.pop(\"nan\", None)\n",
    "            frame_dict.pop(\"blank\", None)\n",
    "            frame_dict.pop(\"out_of_view\", None)\n",
    "            all_frames_predicted_outputs.append(frame_dict) \n",
    "\n",
    "        print(f'List of undefined tools: {set(all_undefined_tools)}.')\n",
    "        tools=sorted(all_frames_predicted_outputs, key=lambda d: d['slice_nr']) \n",
    "\n",
    "        return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0d844-55ff-4905-99f1-b442b17669a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, extracting and cropping video file: vid_1_short.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of undefined tools: {'nan'}.\n",
      "CPU times: user 1min 25s, sys: 51.2 s, total: 2min 16s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%time pred_json=Surgtoolloc_det().predict('vid_1_short.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14e4fe-3213-41f0-a351-6217eed974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53d8cb-81c8-460b-9d6e-61edad49060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if __name__ == \"__main__\":\n",
    "    Surgtoolloc_det().process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaf8bd-d2f6-4730-9d31-306a200e6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_export('09_inference.ipynb', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082be8cd-a9f7-4c1f-a2e7-ae0d7edd413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_export('09_inference.ipynb', '/home/bilal/mlworks/surgtoolloc2022-category-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3a4fd-5d89-48c9-a946-495d3182a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage; print(skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a59352-0ce4-48ac-ae91-2cdfd1d75237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai; print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f5340-5b9d-4c8c-b561-bf6165d624ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
