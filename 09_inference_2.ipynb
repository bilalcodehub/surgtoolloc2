{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6626821-4db1-47bc-8952-8219d9556426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9052d3e-b97d-448d-8d20-69b2772ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531e2ff-02f8-4bb9-8d0d-629de241cb8c",
   "metadata": {},
   "source": [
    "# Install and load all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccc956c-34dd-439a-99d9-eaf25106260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb1c8c2-a278-4a9c-be23-2f2f723fc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b233ca7a-64b9-4799-b02f-0650033bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "from evalutils.validators import UniquePathIndicesValidator, DataFrameValidator\n",
    "from evalutils.exceptions import ValidationError\n",
    "import json, random, SimpleITK, gc, cv2\n",
    "from typing import Tuple, Dict\n",
    "from scipy.ndimage import center_of_mass, label\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0822e98e-a00b-458c-9f6a-3613a6ec71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "execute_in_docker = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9142a2e-75d0-4f02-92ae-3ceaa27dcd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VideoLoader():\n",
    "    def load(self, *, fname):\n",
    "        path = Path(fname)\n",
    "        print(path)\n",
    "        if not path.is_file():\n",
    "            raise IOError(\n",
    "                f\"Could not load {fname} using {self.__class__.__qualname__}.\"\n",
    "            )\n",
    "            #cap = cv2.VideoCapture(str(fname))\n",
    "        #return [{\"video\": cap, \"path\": fname}]\n",
    "        return [{\"path\": fname}]\n",
    "\n",
    "    # only path valid\n",
    "    def hash_video(self, input_video):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8b0ec55-355f-4078-ab12-ea8c070d4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class UniqueVideoValidator(DataFrameValidator):\n",
    "    \"\"\"\n",
    "    Validates that each video in the set is unique\n",
    "    \"\"\"\n",
    "\n",
    "    def validate(self, *, df: DataFrame):\n",
    "        try:\n",
    "            hashes = df[\"video\"]\n",
    "        except KeyError:\n",
    "            raise ValidationError(\"Column `video` not found in DataFrame.\")\n",
    "\n",
    "        if len(set(hashes)) != len(hashes):\n",
    "            raise ValidationError(\"The videos are not unique, please submit a unique video for each case.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3305081-6d9a-4d24-9011-48aeb16ccbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/vid_1_short.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': './test/vid_1_short.mp4'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=VideoLoader()\n",
    "v.load(fname='./test/vid_1_short.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b05c36-ad78-4d2d-b2c8-2a3182257cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Surgtoolloc_det(DetectionAlgorithm):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            index_key='input_video',\n",
    "            file_loaders={'input_video': VideoLoader()},\n",
    "            input_path=Path(\"/input/\") if execute_in_docker else Path(\"./test/\"),\n",
    "            output_file=Path(\"/output/surgical-tool-presence.json\") if execute_in_docker else Path(\n",
    "                \"./output/surgical-tool-presence.json\"),\n",
    "            validators=dict(input_video=(UniquePathIndicesValidator(),)),\n",
    "        )\n",
    "        \n",
    "        # loading ensemble learner\n",
    "        self.ensem_learner=[load_learner(m, cpu=False) for m in Path('/opt/algorithm/cls').ls() if m.suffix=='.pkl']\n",
    "        self.crop_learner=load_learner('/opt/algorithm/seg/seg_v1.pkl', cpu=False)\n",
    "        self.codes = [\"Background\", \"Foreground\"]\n",
    "\n",
    "        self.tool_list = [\"needle_driver\",\n",
    "                          \"monopolar_curved_scissor\",\n",
    "                          \"force_bipolar\",\n",
    "                          \"clip_applier\",\n",
    "                          \"tip_up_fenestrated_grasper\",\n",
    "                          \"cadiere_forceps\",\n",
    "                          \"bipolar_forceps\",\n",
    "                          \"vessel_sealer\",\n",
    "                          \"suction_irrigator\",\n",
    "                          \"bipolar_dissector\",\n",
    "                          \"prograsp_forceps\",\n",
    "                          \"stapler\",\n",
    "                          \"permanent_cautery_hook_spatula\",\n",
    "                          \"grasping_retractor\"]\n",
    "\n",
    "\n",
    "    def get_image_mask(fn):\n",
    "        f=Path(str(fn).replace('images', 'masks').replace('jpg','png'))\n",
    "        return PILMask.create(f) \n",
    "\n",
    "    def custom_accuracy(inp, targ):\n",
    "        targ = targ.squeeze(1)\n",
    "        return (inp.argmax(dim=1)==targ).float().mean()\n",
    "    \n",
    "    def crop_images(src):\n",
    "        fs=get_image_files(src)\n",
    "        preds,_ = self.crop_learner.get_preds(dl=l.dls.test_dl(fs))\n",
    "        for p, f in zip(preds,l.dl.items):\n",
    "\n",
    "            fn = f.name\n",
    "\n",
    "            im=PILImage.create(f)\n",
    "            (h,w)=im.shape\n",
    "            mask=PILMask.create((np.array(p.argmax(0))*255).astype(np.uint8))\n",
    "            mask=Resize((h,w), ResizeMethod.Squish) (mask)\n",
    "\n",
    "            lbl = label(np.array(mask))\n",
    "            props = regionprops(lbl)\n",
    "            x1,y1,x2,y2=props[0].bbox[0],props[0].bbox[2],props[0].bbox[1],props[0].bbox[3]\n",
    "\n",
    "            im_c = PILImage.create(np.array(im)[x1:y1,x2:y2])\n",
    "            im_c.save(src/fn)\n",
    "    \n",
    "    def extract_images(video_file):     \n",
    "        # read the video file    \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        # start the loop\n",
    "        count = 0\n",
    "        src=Path(self._input_file)\n",
    "        while True:\n",
    "            is_read, f = cap.read()\n",
    "            if not is_read:\n",
    "                # break out of the loop if there are no frames to read\n",
    "                break\n",
    "            name = str(src/f'{count}.jpg')\n",
    "            cv2.imwrite(name,f)\n",
    "            count+=1\n",
    "        cap.release()\n",
    "\n",
    "        proc_images(src)\n",
    "    \n",
    "    def tool_detection_model_output(self):\n",
    "        \n",
    "        \n",
    "        random_tool_predictions = [random.randint(0, len(self.tool_list) - 1), random.randint(0, len(self.tool_list) - 1)]\n",
    "\n",
    "        return [self.tool_list[random_tool_predictions[0]], self.tool_list[random_tool_predictions[1]]]\n",
    "\n",
    "    def tool_detect_json_sample(self):\n",
    "        # single output dict\n",
    "        slice_dict = {\"slice_nr\": 1}\n",
    "        tool_boolean_dict = {i: False for i in self.tool_list}\n",
    "\n",
    "        single_output_dict = {**slice_dict, **tool_boolean_dict}\n",
    "\n",
    "        return single_output_dict\n",
    "\n",
    "    def process_case(self, *, idx, case):\n",
    "\n",
    "        # Input video would return the collection of all frames (cap object)\n",
    "        input_video_file_path = case #VideoLoader.load(case)\n",
    "        # Detect and score candidates\n",
    "        scored_candidates = self.predict(case.path) #video file > load evalutils.py\n",
    "\n",
    "        # return\n",
    "        # Write resulting candidates to result.json for this case\n",
    "        return scored_candidates\n",
    "\n",
    "    def save(self):\n",
    "        with open(str(self._output_file), \"w\") as f:\n",
    "            json.dump(self._case_results[0], f)\n",
    "\n",
    "\n",
    "    def predict(self, fname) -> Dict:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        fname -> video file path\n",
    "        \n",
    "        Output:\n",
    "        tools -> list of prediction dictionaries (per frame) in the correct format as described in documentation \n",
    "        \"\"\"\n",
    "        \n",
    "        print('Video file to be loaded: ' + str(fname))\n",
    "        extract_images(fname)\n",
    "        \n",
    "        fs=get_image_files(input_path)\n",
    "        \n",
    "        num_frames = len(fs)\n",
    "        \n",
    "        ###                                                                     ###\n",
    "        ###  TODO: adapt the following part for YOUR submission: make prediction\n",
    "        ###                                                                     ###\n",
    "        \n",
    "        print(num_frames)\n",
    "\n",
    "        # generate output json\n",
    "        all_frames_predicted_outputs = []\n",
    "        all_undefined_tools=[]\n",
    "        \n",
    "        tta_res=[]\n",
    "        prs_items=[]\n",
    "        for learn in ensem_learner:\n",
    "            tta_res.append(learn.tta(dl=learn.dls.test_dl(fs)))\n",
    "            if len(prs_items)<1:\n",
    "                prs_items=learn.dl.items\n",
    "\n",
    "        tta_prs=first(zip(*tta_res))\n",
    "        tta_prs+=tta_prs[1:3]\n",
    "        tta_prs=torch.stack(tta_prs)\n",
    "\n",
    "        lbls=[]\n",
    "        for i in range(len(dls.c)):\n",
    "            arm_preds = tta_prs[:,:,cfg(i):cfg(i+1)].mean(0);\n",
    "            arm_idxs = arm_preds.argmax(dim=1)\n",
    "            arm_vocab = np.array(dls.vocab[i])\n",
    "            lbls.append(arm_vocab[arm_idxs])\n",
    "\n",
    "        for usm1,usm2,usm3,usm4,f in zip(lbls[0],lbls[1],lbls[2],lbls[3],fs_itm):\n",
    "            frame_dict=self.tool_detect_json_sample()\n",
    "            frame_dict['slice_nr']=int(f.stem)\n",
    "            frame_dict[usm1]=True if usm1 in frame_dict.keys() else all_undefined_tools.append(usm1)\n",
    "            frame_dict[usm2]=True if usm2 in frame_dict.keys() else all_undefined_tools.append(usm2)\n",
    "            frame_dict[usm3]=True if usm3 in frame_dict.keys() else all_undefined_tools.append(usm3)\n",
    "            frame_dict[usm4]=True if usm4 in frame_dict.keys() else all_undefined_tools.append(usm4)\n",
    "            frame_dict.pop(\"nan\", None)\n",
    "            frame_dict.pop(\"blank\", None)\n",
    "            frame_dict.pop(\"out_of_view\", None)\n",
    "            all_frames_predicted_outputs.append(frame_dict) \n",
    "\n",
    "        print(f'List of undefined tools: {set(all_undefined_tools)}.')\n",
    "        tools=sorted(all_frames_predicted_outputs, key=lambda d: d['slice_nr']) \n",
    "\n",
    "        return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0d844-55ff-4905-99f1-b442b17669a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Surgtoolloc_det.process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0f9d1-00ae-4d29-b044-4d7dc01f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if __name__ == \"__main__\":\n",
    "    Surgtoolloc_det().process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaf8bd-d2f6-4730-9d31-306a200e6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_export('09_inference_2.ipynb', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60629d8-949d-49d2-9b95-15a1bfebc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=('a','b','c')\n",
    "for l in lst:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f662d-47f8-4afd-9af8-97756ac99283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_learner=[load_learner(m,cpu=False) for m in Path('models/small').ls() if m.suffix=='.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adf5ea-a633-453e-92b6-b87943b2e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(ensemble_learner)} model(s) in this ensemble learner.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60f7f1-8e84-4c77-bf39-94631275324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0e729-7185-44de-ac6f-ccdc69506cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [\"Background\", \"Foreground\"]\n",
    "\n",
    "def get_image_mask(fn):\n",
    "    f=Path(str(fn).replace('images', 'masks').replace('jpg','png'))\n",
    "    return PILMask.create(f) \n",
    "\n",
    "def custom_accuracy(inp, targ):\n",
    "    targ = targ.squeeze(1)\n",
    "    return (inp.argmax(dim=1)==targ).float().mean()\n",
    "\n",
    "def proc_images(src):\n",
    "    l=load_learner('/home/bilal/mlworks/surgtoolloc/models/seg/seg_v1.pkl', cpu=False)\n",
    "    fs=get_image_files(src)\n",
    "    preds,_ = l.get_preds(dl=l.dls.test_dl(fs))\n",
    "    for p, f in zip(preds,l.dl.items):\n",
    "\n",
    "        fn = f.name\n",
    "\n",
    "        im=PILImage.create(f)\n",
    "        (h,w)=im.shape\n",
    "        mask=PILMask.create((np.array(p.argmax(0))*255).astype(np.uint8))\n",
    "        mask=Resize((h,w), ResizeMethod.Squish) (mask)\n",
    "\n",
    "        lbl = label(np.array(mask))\n",
    "        props = regionprops(lbl)\n",
    "        x1,y1,x2,y2=props[0].bbox[0],props[0].bbox[2],props[0].bbox[1],props[0].bbox[3]\n",
    "\n",
    "        im_c = PILImage.create(np.array(im)[x1:y1,x2:y2])\n",
    "        im_c.save(src/fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b36f8b-89a0-467c-8cfc-48b4690e9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(video_file):     \n",
    "    # read the video file    \n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    # start the loop\n",
    "    count = 0\n",
    "    src=Path('test_data/input/')\n",
    "    while True:\n",
    "        is_read, f = cap.read()\n",
    "        if not is_read:\n",
    "            # break out of the loop if there are no frames to read\n",
    "            break\n",
    "        name = str(src/f'{count}.jpg')\n",
    "        cv2.imwrite(name,f)\n",
    "        count+=1\n",
    "    cap.release()\n",
    "    \n",
    "    proc_images(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43969a76-7506-4840-8552-05a275dbd2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(video_file):\n",
    "    \n",
    "    extract_images(video_file)\n",
    "    \n",
    "    pred_dict = {\n",
    "        \"slice_nr\": -1,\n",
    "        \"needle_driver\": False,\n",
    "        \"monopolar_curved_scissors\": False,\n",
    "        \"force_bipolar\": False,\n",
    "        \"clip_applier\": False,\n",
    "        \"tip_up_fenestrated_grasper\": False,\n",
    "        \"cadiere_forceps\": False,\n",
    "        \"bipolar_forceps\": False,\n",
    "        \"vessel_sealer\": False,\n",
    "        \"suction_irrigator\": False,\n",
    "        \"bipolar_dissector\": False,\n",
    "        \"prograsp_forceps\": False,\n",
    "        \"stapler\": False,\n",
    "        \"permanent_cautery_hook_spatula\": False,\n",
    "        \"grasping_retractor\": False\n",
    "    }\n",
    "\n",
    "    ignored=[]\n",
    "    video_output=[]\n",
    "    inp = 'test_data/input'\n",
    "    out = 'test_data/output'\n",
    "    \n",
    "    fs=get_image_files(inp)\n",
    "    \n",
    "\n",
    "    \n",
    "    tta_res=[]\n",
    "    fs_itm=[]\n",
    "    for learn in ensemble_learner:\n",
    "        tta_res.append(learn.tta(dl=learn.dls.test_dl(fs)))\n",
    "        if len(fs_itm)<1:\n",
    "            fs_itm=learn.dl.items\n",
    "    \n",
    "    tta_prs=first(zip(*tta_res))\n",
    "#     tta_prs+=tta_prs[1:3]\n",
    "    tta_prs=torch.stack(tta_prs)\n",
    "    \n",
    "    lbls=[]\n",
    "    for i in range(len(dls.c)):\n",
    "        arm_preds = tta_prs[:,:,cfg(i):cfg(i+1)].mean(0);\n",
    "        arm_idxs = arm_preds.argmax(dim=1)\n",
    "        arm_vocab = np.array(dls.vocab[i])\n",
    "        lbls.append(arm_vocab[arm_idxs])\n",
    "    \n",
    "    for usm1,usm2,usm3,usm4,f in zip(lbls[0],lbls[1],lbls[2],lbls[3],fs_itm):\n",
    "        print(usm1,usm2,usm3,usm4,f)\n",
    "        a_pred_dict=copy.deepcopy(pred_dict)\n",
    "        a_pred_dict['slice_nr']=int(f.stem)\n",
    "        a_pred_dict[usm1]=True if usm1 in a_pred_dict.keys() else ignored.append(usm1)\n",
    "        a_pred_dict[usm2]=True if usm2 in a_pred_dict.keys() else ignored.append(usm2)\n",
    "        a_pred_dict[usm3]=True if usm3 in a_pred_dict.keys() else ignored.append(usm3)\n",
    "        a_pred_dict[usm4]=True if usm4 in a_pred_dict.keys() else ignored.append(usm4)\n",
    "        a_pred_dict.pop(\"nan\", None)\n",
    "        a_pred_dict.pop(\"blank\", None)\n",
    "        video_output.append(copy.deepcopy(a_pred_dict))   \n",
    "        \n",
    "    video_output=sorted(video_output, key=lambda d: d['slice_nr']) \n",
    "    print(set(ignored))\n",
    "    \n",
    "    with open('test_data/output/surgical-tool-presence.json', 'w') as fn:\n",
    "        json.dump(video_output, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661330c-2e55-4574-aab0-2256d3a8ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file='test_data/vid_1_short.mp4'\n",
    "predict(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c8437-c1dd-4c66-a173-fe91411576e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
